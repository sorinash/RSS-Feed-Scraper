# RSS-Feed-Scraper
A tool intended to demonstrate RSS feed scraping, along with usage of BeautifulSoup

My script scrapes the RSS feeds for in-press articles for several Cell-affiliated journals, namely Current Biology, Cell Systems, Neuron, Cell Metabolism, and Trends in Endocrinology and Metabolism. 

For each feed, it makes use of the method 'find_relevant_articles'. When given a URL and a list of keywords, the script searches through the articles in the provided RSS feeds. It isolates individual articles by finding bits of the XML with the tag "item". It then defines a lambda function which isolates the description from an item by performing a findall for the tag 'description', and checks to see if said description contains a word from my list of keywords (such as 'Anxiety' or 'Estrogen'). It then applies this lambda function to the list of articles. If it finds an article that causes the lambda function to return a number of True values greater than a certain threshold (kept at 1 for now, as there were comparatively few articles discussing things from the keyword list), it adds that article to a list of relevant articles to search. The method then returns a list of ALL The articles, as well as a list of only the relevant articles. It then adds both lists from an individual RSS feed to a collective list for total and relevant articles, respectively. 

After scraping every provided RSS feed, the script passes the list of relevant articles to a method called "get_those_sources" along with the list of keywords. "get_those_sources" goes through each article. It uses find_all("dc:title") to isolate the title of each article and use that as a key in a dictionary. It then uses find_all("link") to get the article's URL. The URL is converted to a string and removes the irrelevant information, namely the tags around the url. It then uses requests.get to request the article from the URL without triggering a 403 error. After that, it uses time.sleep(2) to pause for 2 seconds. This pause is included to avoid triggering a response to overenthusiastic requests/DDOS attacks, and, in turn, avoid getting blocked. 

From there, "get_those_sources"  then converts the article into a beautiful-soup object, and isolates everything from the xml with the tag "meta", provided that "citation_title=" is in the tag. This allows me to isolate individual citations. Within each citation tag, the script then separates items out by equals signs. This results in a list of strings where the article title, journal name, and primary author are all in predictable positions. This likely doesn't apply to single-author articles based on the xml formatting in Cell articles, but those are so rare at this point that I'm ignoring them for the purposes of this homework assignment. Within the title position, the method splits the strings up by ";", allowing me to isolate only the relevant text for those bits of information, such as the article's title. It then searches the title for the keywords, and, if any show up, creates a list containing the title, journal name, and primary author. It then adds that list to the relevant key in the dictionary. When this is completed for all articles, it then returns the dictionary. 

From there, this dictionary gets passed to prettify_titles_and_citations, which creates formats the dictionary to be easier on the eyes. In brief, it starts off with each dict key and, within each dict key, processes each list so that the title, journal and author are all on different lines and with readable formatting. While doing so, it counts the number of cited articles within all the searched articles, and returns how many relevant articles there are and how many cited articles there are.

This script will only function for Cell-affiliated sites, or at least RSS feeds for sites that are formatted in a similar manner. 
